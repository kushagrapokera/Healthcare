{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3b97d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (4.50.0)\n",
      "Requirement already satisfied: torch in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc0d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026df0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kusha\\.cache\\huggingface\\hub\\models--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5e71ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (44.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
      "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.6 MB 985.5 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 985.5 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 985.5 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.0/5.6 MB 949.8 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.3/5.6 MB 906.3 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.3/5.6 MB 906.3 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.3/5.6 MB 906.3 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 798.7 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 798.7 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 798.7 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.8/5.6 MB 675.6 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.8/5.6 MB 675.6 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.8/5.6 MB 675.6 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.8/5.6 MB 675.6 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.8/5.6 MB 675.6 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.1/5.6 MB 551.4 kB/s eta 0:00:07\n",
      "   -------------- ------------------------- 2.1/5.6 MB 551.4 kB/s eta 0:00:07\n",
      "   -------------- ------------------------- 2.1/5.6 MB 551.4 kB/s eta 0:00:07\n",
      "   -------------- ------------------------- 2.1/5.6 MB 551.4 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 2.4/5.6 MB 491.6 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 2.4/5.6 MB 491.6 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 2.4/5.6 MB 491.6 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 2.6/5.6 MB 490.2 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 2.6/5.6 MB 490.2 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 2.9/5.6 MB 497.8 kB/s eta 0:00:06\n",
      "   -------------------- ------------------- 2.9/5.6 MB 497.8 kB/s eta 0:00:06\n",
      "   -------------------- ------------------- 2.9/5.6 MB 497.8 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 483.1 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 483.1 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 483.1 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 3.4/5.6 MB 479.3 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 3.4/5.6 MB 479.3 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 3.4/5.6 MB 479.3 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 3.7/5.6 MB 483.6 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 3.7/5.6 MB 483.6 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 3.9/5.6 MB 491.4 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 3.9/5.6 MB 491.4 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 3.9/5.6 MB 491.4 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 480.2 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 480.2 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 480.2 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 4.5/5.6 MB 476.8 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 4.5/5.6 MB 476.8 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 4.5/5.6 MB 476.8 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 4.5/5.6 MB 476.8 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 4.7/5.6 MB 469.1 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.7/5.6 MB 469.1 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.7/5.6 MB 469.1 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.0/5.6 MB 465.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.0/5.6 MB 465.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.0/5.6 MB 465.3 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.2/5.6 MB 460.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.6 MB 460.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.6 MB 460.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.6 MB 460.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.6 MB 452.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 454.0 kB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/3.0 MB 381.6 kB/s eta 0:00:07\n",
      "   ------- -------------------------------- 0.5/3.0 MB 381.6 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 435.8 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 435.8 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 435.8 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 1.0/3.0 MB 466.2 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 1.0/3.0 MB 466.2 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 1.3/3.0 MB 497.2 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 1.6/3.0 MB 534.3 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 1.6/3.0 MB 534.3 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 1.8/3.0 MB 550.1 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 1.8/3.0 MB 550.1 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 1.8/3.0 MB 550.1 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 551.4 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 551.4 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 551.4 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 2.4/3.0 MB 536.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 2.4/3.0 MB 536.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 2.4/3.0 MB 536.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 2.4/3.0 MB 536.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 491.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 491.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 491.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 491.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 491.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 491.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.9/3.0 MB 433.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 427.4 kB/s eta 0:00:00\n",
      "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b96397",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = (\"\"\"As your baby gets a bit older, she won't need as many feeds during the night. As such, she'll be able to sleep for a bit longer. By this stage, some babies will even sleep for a full eight hours during the night (although, word of warning: don't get your hopes up). Most babies will spend twice as long sleeping at night as they do during the day – e.g. eight hours at night and four hours during the day.\n",
    "Many babies aged six months to a year will no longer need a night feed and may sleep for up to 12 hours at night. However, there're no guarantees of this, and some babies may wake up in the night for a bit (or a lot) longer.\n",
    "Sometimes, your baby will nod off with seemingly no effort required from you. No rocking, no shushing, no patting. Then there are the nights where your baby will fight sleep as though their little life depends on it. What's more, once she does drop off, it won't be long before she's awake again. You know she needs to sleep, but she hasn't quite got the memo.\n",
    "\n",
    "For nights like these, we've compiled a guide for getting your baby to sleep. Whether you give controlled crying a go, or experiment with using a dummy, there're a number of things you can do to up your chances of getting your baby to sleep.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e18015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As your baby gets a bit older, she won't need as many feeds during the night. As such, she'll be able to sleep for a bit longer. By this stage, some babies will even sleep for a full eight hours during the night (although, word of warning: don't get your hopes up). Most babies will spend twice as long sleeping at night as they do during the day – e.g. eight hours at night and four hours during the day.\\nMany babies aged six months to a year will no longer need a night feed and may sleep for up to 12 hours at night. However, there're no guarantees of this, and some babies may wake up in the night for a bit (or a lot) longer.\\nSometimes, your baby will nod off with seemingly no effort required from you. No rocking, no shushing, no patting. Then there are the nights where your baby will fight sleep as though their little life depends on it. What's more, once she does drop off, it won't be long before she's awake again. You know she needs to sleep, but she hasn't quite got the memo.\\n\\nFor nights like these, we've compiled a guide for getting your baby to sleep. Whether you give controlled crying a go, or experiment with using a dummy, there're a number of things you can do to up your chances of getting your baby to sleep.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c08cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"how many hours do the six month old babies sleep?\",\n",
    "    \"how to make my baby sleep?\",\n",
    "    \"what are the traits of children with autism?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299cdba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how many hours do the six month old babies sleep?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32e1a5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kusha\\.cache\\huggingface\\hub\\models--deepset--bert-base-cased-squad2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66cd3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f039c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('deepset/bert-base-cased-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "127e1c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1293, 1106, 1294, 1139, 2963, 2946, 136, 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(questions[1] , truncation=True , padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb564ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f632b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline('question-answering' , model = model , tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a669bb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.32852479815483093, 'start': 501, 'end': 503, 'answer': '12'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp({\n",
    "    'question': questions[0],\n",
    "    'context': context\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32e21d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0396769680082798,\n",
       " 'start': 31,\n",
       " 'end': 59,\n",
       " 'answer': \"she won't need as many feeds\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp({\n",
    "    'question': questions[1],\n",
    "    'context': context\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ca158a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.958210411132313e-05,\n",
       " 'start': 406,\n",
       " 'end': 443,\n",
       " 'answer': 'Many babies aged six months to a year'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp({\n",
    "    'question': questions[2],\n",
    "    'context': context\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f719cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kusha\\anaconda3\\envs\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f21587ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how many hours do six-month-old babies sleep?\n",
      "Answer: 12 hours\n",
      "\n",
      "Question: how can I help my baby sleep?\n",
      "Answer: experiment with using a dummy\n",
      "\n",
      "Question: what are the signs of autism in children?\n",
      "Answer: No \n",
      "rocking, no shushing, no patting\n",
      "\n",
      "Input IDs: [101, 2129, 2116, 2847, 2079, 2416, 1011, 3204, 1011, 2214, 10834, 3637, 1029, 102, 2004, 2115, 3336, 4152, 1037, 2978, 3080, 1010, 2016, 2180, 1005, 1056, 2342, 2004, 2116, 14172, 2076, 1996, 2305, 1012, 2004, 2107, 1010, 2016, 1005, 2222, 2022, 2583, 2000, 3637, 2005, 1037, 2978, 2936, 1012, 2011, 2023, 2754, 1010, 2070, 10834, 2097, 2130, 3637, 2005, 1037, 2440, 2809, 2847, 2076, 1996, 2305, 1006, 2348, 1010, 2773, 1997, 5432, 1024, 2123, 1005, 1056, 2131, 2115, 8069, 2039, 1007, 1012, 2087, 10834, 2097, 5247, 3807, 2004, 2146, 5777, 2012, 2305, 2004, 2027, 2079, 2076, 1996, 2154, 1516, 1041, 1012, 1043, 1012, 2809, 2847, 2012, 2305, 1998, 2176, 2847, 2076, 1996, 2154, 1012, 2116, 10834, 4793, 2416, 2706, 2000, 1037, 2095, 2097, 2053, 2936, 2342, 1037, 2305, 5438, 1998, 2089, 3637, 2005, 2039, 2000, 2260, 2847, 2012, 2305, 1012, 2174, 1010, 2045, 1005, 2128, 2053, 21586, 1997, 2023, 1010, 1998, 2070, 10834, 2089, 5256, 2039, 1999, 1996, 2305, 2005, 1037, 2978, 1006, 2030, 1037, 2843, 1007, 2936, 1012, 2823, 1010, 2115, 3336, 2097, 7293, 2125, 2007, 9428, 2053, 3947, 3223, 2013, 2017, 1012, 2053, 14934, 1010, 2053, 18454, 12227, 1010, 2053, 26085, 1012, 2059, 2045, 2024, 1996, 6385, 2073, 2115, 3336, 2097, 2954, 3637, 2004, 2295, 2037, 2210, 2166, 9041, 2006, 2009, 1012, 2054, 1005, 1055, 2062, 1010, 2320, 2016, 2515, 4530, 2125, 1010, 2009, 2180, 1005, 1056, 2022, 2146, 2077, 2016, 1005, 1055, 8300, 2153, 1012, 2017, 2113, 2016, 3791, 2000, 3637, 1010, 2021, 2016, 8440, 1005, 1056, 3243, 2288, 1996, 24443, 1012, 2005, 6385, 2066, 2122, 1010, 2057, 1005, 2310, 9227, 1037, 5009, 2005, 2893, 2115, 3336, 2000, 3637, 1012, 3251, 2017, 2507, 4758, 6933, 1037, 2175, 1010, 2030, 7551, 2007, 2478, 1037, 24369, 1010, 2045, 1005, 2128, 1037, 2193, 1997, 2477, 2017, 2064, 2079, 2000, 2039, 2115, 9592, 1997, 2893, 2115, 3336, 2000, 3637, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Define the context and questions\n",
    "context = \"\"\"As your baby gets a bit older, she won't need as many feeds during the night.\n",
    "As such, she'll be able to sleep for a bit longer. By this stage, some babies will even sleep for a full eight hours during the\n",
    "night (although, word of warning: don't get your hopes up). Most babies will spend twice as long sleeping at night as they do\n",
    "during the day – e.g. eight hours at night and four hours during the day.\\nMany babies aged six months to a year will no longer\n",
    "need a night feed and may sleep for up to 12 hours at night. However, there're no guarantees of this, and some babies may wake \n",
    "up in the night for a bit (or a lot) longer.\\nSometimes, your baby will nod off with seemingly no effort required from you. No \n",
    "rocking, no shushing, no patting. Then there are the nights where your baby will fight sleep as though their little life depends\n",
    "on it. What's more, once she does drop off, it won't be long before she's awake again. You know she needs to sleep, but she \n",
    "hasn't quite got the memo.\\n\\nFor nights like these, we've compiled a guide for getting your baby to sleep. Whether you give \n",
    "controlled crying a go, or experiment with using a dummy, there're a number of things you can do to up your chances of getting\n",
    "your baby to sleep.\"\"\"  # Replace with your context\n",
    "questions = [\n",
    "    \"how many hours do six-month-old babies sleep?\",\n",
    "    \"how can I help my baby sleep?\",\n",
    "    \"what are the signs of autism in children?\",\n",
    "]\n",
    "\n",
    "# Load the question-answering pipeline\n",
    "nlp = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad', tokenizer='bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Process each question and get answers\n",
    "for question in questions:\n",
    "    answer = nlp({\n",
    "        'question': question,\n",
    "        'context': context\n",
    "    })\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer['answer']}\\n\")\n",
    "\n",
    "# Ensure you have the correct tokenizer for your model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Check tokenization and encoding\n",
    "input_ids = tokenizer.encode(questions[0], context, truncation=True, padding='max_length', max_length=512)\n",
    "print(\"Input IDs:\", input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f0b2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39731dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73de5d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fadae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
    "                                    return_dict=False) #Huggingface version update required the dictionary return to be False\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fb27f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As your baby gets a bit older, she won't need as many feeds during the night. As\n",
      "such, she'll be able to sleep for a bit longer. By this stage, some babies will\n",
      "even sleep for a full eight hours during the night (although, word of warning:\n",
      "don't get your hopes up). Most babies will spend twice as long sleeping at night\n",
      "as they do during the day – e.g. eight hours at night and four hours during the\n",
      "day. Many babies aged six months to a year will no longer need a night feed and\n",
      "may sleep for up to 12 hours at night. However, there're no guarantees of this,\n",
      "and some babies may wake up in the night for a bit (or a lot) longer. Sometimes,\n",
      "your baby will nod off with seemingly no effort required from you. No rocking,\n",
      "no shushing, no patting. Then there are the nights where your baby will fight\n",
      "sleep as though their little life depends on it. What's more, once she does drop\n",
      "off, it won't be long before she's awake again. You know she needs to sleep, but\n",
      "she hasn't quite got the memo.  For nights like these, we've compiled a guide\n",
      "for getting your baby to sleep. Whether you give controlled crying a go, or\n",
      "experiment with using a dummy, there're a number of things you can do to up your\n",
      "chances of getting your baby to sleep.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "context1 = \"\"\"As your baby gets a bit older, she won't need as many feeds during the night. As such, she'll be able to sleep for a bit longer. By this stage, some babies will even sleep for a full eight hours during the night (although, word of warning: don't get your hopes up). Most babies will spend twice as long sleeping at night as they do during the day – e.g. eight hours at night and four hours during the day.\n",
    "Many babies aged six months to a year will no longer need a night feed and may sleep for up to 12 hours at night. However, there're no guarantees of this, and some babies may wake up in the night for a bit (or a lot) longer.\n",
    "Sometimes, your baby will nod off with seemingly no effort required from you. No rocking, no shushing, no patting. Then there are the nights where your baby will fight sleep as though their little life depends on it. What's more, once she does drop off, it won't be long before she's awake again. You know she needs to sleep, but she hasn't quite got the memo.\n",
    "\n",
    "For nights like these, we've compiled a guide for getting your baby to sleep. Whether you give controlled crying a go, or experiment with using a dummy, there're a number of things you can do to up your chances of getting your baby to sleep.\"\"\"\n",
    "\n",
    "print(wrapper.fill(context1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e737772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 309 tokens.\n",
      "\n",
      "Answer: \"12 hours at night\"\n"
     ]
    }
   ],
   "source": [
    "question = \"how many hours do the six month old babies sleep?\"\n",
    "\n",
    "answer_question(question, context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5fd1c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 305 tokens.\n",
      "\n",
      "Answer: \"whether you give controlled crying a go , or experiment with using a dummy\"\n"
     ]
    }
   ],
   "source": [
    "question = \"how to make my baby sleep?\"\n",
    "\n",
    "answer_question(question, context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba1766dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Children with Autism Spectrum Disorder are often restricted, rigid, and even\n",
      "obsessive in their behaviors, activities, and interests. Symptoms may include:\n",
      "Repetitive body movements (hand flapping, rocking, spinning); moving constantly.\n",
      "Obsessive attachment to unusual objects (rubber bands, keys, light switches).\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "context = \"\"\"Children with Autism Spectrum Disorder are often restricted, rigid, and even obsessive in their behaviors, activities, and interests. Symptoms may include: Repetitive body movements (hand flapping, rocking, spinning); moving constantly. Obsessive attachment to unusual objects (rubber bands, keys, light switches).\"\"\"\n",
    "\n",
    "print(wrapper.fill(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0602903c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 74 tokens.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"restricted , rigid , and even obsessive\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"what are the traits of children with autism?\"\n",
    "\n",
    "answer_question(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17012099",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_text = \"\"\"ASD stands for Autism Spectrum Disorder. \"Autism\" in the terminology refers to the defects in communication (speaking/ gesturing/ listening; any mode of communication to get the message across to other person); socialization (how the individual fits into a group like the family, friends or community) and limited interests and/ or repetitive behavior. This has been dealt in detail in subsequent paragraphs. The \"spectrum\" part of terminology denotes that each and every person with this condition is different and unique. While there may be many differences among children in different parts of spectrum, the key difference is when these children began to talk and learn. If they are talking by the age of 3 years and do not have difficulty in learning, the child may be labelled to have Asperger's, while others with delayed language milestones and impaired learning would end up on the severe end of spectrum. The earlier DSM IV criteria suggested the following sub-classification of autism; however, the recent DSM-5 has merged these and assimilated them under the umbrella term of ASD.\n",
    "Even though ASD can be diagnosed as early as age 2 years, most children are not diagnosed with ASD until after age 4 years. The median age of first diagnosis by subtype is as follows: • Autistic disorder: 3 years 10 months • Pervasive developmental disorder-not otherwise specified (PDD-NOS): 4 years 1 month • Asperger disorder: 6 years 2 months Studies have shown that parents of children with ASD notice a developmental problem before their child’s first birthday. Concerns about vision and hearing were more often reported in the first year, and differences in social, communication, and fine motor skills were evident from 6 months of age. And further research has shown that if intervened early, these children may be able to overcome some of their socio-behavioral handicaps. Hence, currently there is an endeavor to identify these children \"at risk\" early i.e., within the first one to two years of life and introduce therapeutic interventions. This involves educating the parents about the red flags and also training the physicians to specifically ask and check for specific symptoms/ signs. This identification of autism spectrum disorders at an age of one to two years is referred to as \"early identification\" of autism.\n",
    "Autism has an onset at an early age, usually infancy, but it may go unnoticed because the signs are very subtle and non-specific. The important early cues that may aid diagnosis of autism include: Figure 1: Classification of ASD. Infancy: (a) Infant not responding to cuddliness (b) Infant shying away from being picked up and held (c) Failure to make eye contact or look at people - especially at parents – when they are talking to them or interacting with them. (d) Failure to look at or acknowledge a toy or item when it is shown to them. Toddlers: (e) Toddlers not engaging in imaginative play. Rather than pretending with their toys or other children, they may meticulously line up dolls, cars, and other items and become upset when someone disturbs the line. (f) Toddlers have difficulties sharing their toys/ items (g) Toddlers may not express any interest in a particular item even if that toy or doll is their favorite (h) As toddlers, they may become fixated on specific objects or details of the objects, which may range from certain toys, certain items of clothing, or even the way sunlight hits the leaves outside the window. Spinning of wheels is a common obsession. But tactile obsessions may also be seen when the child may constantly rub soft or smooth items across their cheeks/ lips/ hands. (i) Toddler may be unresponsive. This means he/ she does not respond when his/ her name is called out. (j) Toddler may have repetitive action. They may watch the same movie over and over; rock back and forth continuously.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aad9b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "406c4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95a02270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3a6ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
    "                                    return_dict=False) #Huggingface version update required the dictionary return to be False\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e864f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASD stands for Autism Spectrum Disorder. “Autism” in the terminology refers to\n",
      "the defects in communication (speaking/ gesturing/ listening; any mode of\n",
      "communication to get the message across to other person); socialization (how the\n",
      "individual fits into a group like the family, friends or community) and limited\n",
      "interests and/ or repetitive behavior. This has been dealt in detail in\n",
      "subsequent paragraphs. The “spectrum” part of terminology denotes that each and\n",
      "every person with this condition is different and unique. While there may be\n",
      "many differences among children in different parts of spectrum, the key\n",
      "difference is when these children began to talk and learn. If they are talking\n",
      "by the age of 3 years and do not have difficulty in learning, the child may be\n",
      "labelled to have Asperger’s, while others with delayed language milestones and\n",
      "impaired learning would end up on the severe end of spectrum. The earlier DSM IV\n",
      "criteria suggested the following sub-classification of autism; however, the\n",
      "recent DSM-5 has merged these and assimilated them under the umbrella term of\n",
      "ASD Even though ASD can be diagnosed as early as age 2 years, most children are\n",
      "not diagnosed with ASD until after age 4 years. The median age of first\n",
      "diagnosis by subtype is as follows : • Autistic disorder : 3 years 10 months •\n",
      "Pervasive developmental disorder-not otherwise specified (PDD-NOS): 4 years 1\n",
      "month • Asperger disorder : 6 years 2 months Studies have shown that parents of\n",
      "children with ASD notice a developmental problem before their child’s first\n",
      "birthday. Concerns about vision and hearing were more often reported in the\n",
      "first year, and differences in social, communication, and fine motor skills were\n",
      "evident from 6 months of age. And further research has shown that if intervened\n",
      "early, these children may be able to overcome some of their socio-behavioral\n",
      "handicaps. Hence, currently there is an endeavor to identify these children “at\n",
      "risk” early i.e., within first one to two years of life and introduce\n",
      "therapeutic interventions. This involves educating the parents about the red\n",
      "flags and also train the physicians to specifically ask and check for specific\n",
      "symptoms/ signs. This identification of autism spectrum disorders at an age of\n",
      "one to two years is referred to as “early identification” of autism Autism has\n",
      "an onset at an early age usually infancy, but it may go unnoticed because the\n",
      "signs are very subtle and non-specific. The important early cues that may aid\n",
      "diagnosis of autism include: Figure 1: Classification of ASD 41 Infancy (a)\n",
      "Infant not responding to cuddliness (b) Infant shying away from being picked up\n",
      "and held (c) Failure to make eye contact or look at people - especially at\n",
      "parents – when they are talking to them or interacting with them. (d) Failure to\n",
      "look at or acknowledge a toy or item when it is shown to them. Toddlers (e)\n",
      "Toddlers not engaging in imaginative play. Rather than pretending with their\n",
      "toys or other children, they may meticulously line up dolls, cars, and other\n",
      "items and become upset when someone disturbs the line. (f) Toddlers have\n",
      "difficulties sharing their toys/ items (g) Toddlers may not express any interest\n",
      "in a particular item even if that toy or doll is their favorite (h) As toddlers,\n",
      "thay may become fixated on specific objects or details of the objects, which may\n",
      "range from certain toys, certain items of clothing, or even the way sunlight\n",
      "hits the leaves outside the window. Spinning of wheels is a common obsession.\n",
      "But tactile obsessions may also be seen when the child may constantly rub soft\n",
      "or smooth items across their cheeks/ lips/ hand. (i) Toddler may be\n",
      "unresponsive. This means he/ she does not respond when his/ her name is called\n",
      "out. (j) Toddler may have repetitive action.They may watch the same movie over\n",
      "and over; rock back and forth continuously. Autism is quite different from most\n",
      "childhood ailments, for which, taking a blood sample or or doing an imaging test\n",
      "can determine the diagnosis. In autism, there is nothing organically wrong with\n",
      "the brain- it is not diseased or structurally different; rather autism is a\n",
      "malfunction in the way brain acts, so the only way to diagnose it is to observe\n",
      "a child’s behavior. Hence the diagnosis of autism is subjective, based on the\n",
      "answers a parent gives to a series of questions, and the observations of the\n",
      "evaluator. The one constant in a diagnosis is the criteria used: the Diagnostic\n",
      "and Statistical Manual of Mental Disorders, developed by the American\n",
      "Psychiatric Association and commonly referred to as DSM criteria. These DSM\n",
      "criteria have evolved over time, and currently DSM-5 criteria are used for\n",
      "diagnosing Autism Spectrum Disorders The fifth edition of Diagnostic and\n",
      "Statistical Manual of Mental Disorders (DSM-5) released in May 2013 does not\n",
      "sub-classify children according to Rett’s disorder, Childhood Disintegrative\n",
      "Disorder, Aspergers Disorder, or Pervasive Developmental Disorder (Not Otherwise\n",
      "Specified). It categorizes every child as ASD but with different levels of\n",
      "severity. These have been illustrated in table 1 and 2. The tests that are used\n",
      "for autism screening include : (a) CHecklist of Autism in Toddlers (CHAT) (b)\n",
      "Modified CHecklist of Autism in Toddlers (M-CHAT) (c) Screening Tool for Autism\n",
      "in Two-year olds (STAT) (d) Social Communication Questionnaire (SCQ) The tests\n",
      "that may be used in autism assessment include : 42 (a) Childhood Autism Rating\n",
      "Scale (CARS) (b) Autism Behavior Checklist (ABC) (c) Child Behavior Check List\n",
      "(CBCL) (d) Autism Treatment Evaluation Checklist (ATEC) (e) Developmental\n",
      "Profile 3 Persistent deficits in social communication and social interaction\n",
      "across multiple contexts, as manifested by the following criteria, currently or\n",
      "by history : 1. Deficits in social-emotional reciprocity, ranging, for example,\n",
      "from abnormal social approach and failure of normal back-and-forth conversation;\n",
      "to reduced sharing of interests, emotions, or affect; to failure to initiate or\n",
      "respond to social interactions. 2. Deficits in non-verbal communicative\n",
      "behaviors used for social interaction, ranging, for example, from poorly\n",
      "integrated verbal and non-verbal communication; to abnormalities in eye contact\n",
      "and body language or deficits in understanding and use of gestures; to a total\n",
      "lack of facial expressions and non-verbal communication. 3. Deficits in\n",
      "developing, maintaining and understanding relationships, ranging, for example,\n",
      "from difficulties adjusting behavior to suit various social contexts; to\n",
      "difficulties in sharing imaginative play or in making friends; to absence of\n",
      "interest in peers. B. Restricted, repetitive patterns of behavior, interests or\n",
      "activities, as manifested by at least two of the following, currently or by\n",
      "history: 1. Stereotyped or repetitive motor movements, use of objects, or speech\n",
      "(example simple motor stereotypies, lining up toys or flipping objects,\n",
      "echolalia, idiosyncratic phrases). 2. Insistence on sameness, inflexible\n",
      "adherence to routines or ritualized patterns or verbal/ non-verbal behavior\n",
      "(e.g., extreme distress at small changes, difficulties with transitions, rigid\n",
      "thinking patterns, greeting rituals, need to take the same route or eat same\n",
      "food everyday) 3. Highly restricted, fixated interests that are abnormal in\n",
      "intensity or focus (e.g., strong attachment to or preoccupation with unusual\n",
      "objects, excessively circumscribed or preservative interest). 4. Hyper- or hypo-\n",
      "reactivity to sensory input or unusual interests in sensory aspects of the\n",
      "environment (e.g., apparent indifference to pain/ temperature, adverse response\n",
      "to specific sounds or textures, excessive smelling or touching of objects,\n",
      "visual fascination with lights or movements) C. Symptoms must be present in the\n",
      "early developmental period (but may not become fully manifested until social\n",
      "demands exceed limited capacities, or may be masked by learned strategies in\n",
      "later life). D. Symptoms cause clinically significant impairment in social,\n",
      "occupational or other important areas of current functioning. E. These\n",
      "disturbances are not better explained by intellectual disability (intellectual\n",
      "developmental disorder) or global developmental delay. Intellectual disability\n",
      "and autism spectrum disorder frequently co-occur; to make co-morbid diagnosis of\n",
      "autism spectrum disorder and intellectual disability, social communication\n",
      "should be below that expected for general developmental level\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "context1 = \"\"\"ASD stands for Autism Spectrum Disorder. “Autism” in the terminology refers to the defects in communication (speaking/ gesturing/ listening; any mode of communication to get the message across to other person); socialization (how the individual fits into a group like the family, friends or community) and limited interests and/ or repetitive behavior. This has been dealt in detail in subsequent paragraphs. The “spectrum” part of terminology denotes that each and every person with this condition is different and unique. While there may be many differences among children in different parts of spectrum, the key difference is when these children began to talk and learn. If they are talking by the age of 3 years and do not have difficulty in learning, the child may be labelled to have Asperger’s, while others with delayed language milestones and impaired learning would end up on the severe end of spectrum. The earlier DSM IV criteria suggested the following sub-classification of autism; however, the recent DSM-5 has merged these and assimilated them under the umbrella term of ASD\n",
    "Even though ASD can be diagnosed as early as age 2 years, most children are not diagnosed with ASD until after age 4 years. The median age of first diagnosis by subtype is as follows : • Autistic disorder : 3 years 10 months • Pervasive developmental disorder-not otherwise specified (PDD-NOS): 4 years 1 month • Asperger disorder : 6 years 2 months Studies have shown that parents of children with ASD notice a developmental problem before their child’s first birthday. Concerns about vision and hearing were more often reported in the first year, and differences in social, communication, and fine motor skills were evident from 6 months of age. And further research has shown that if intervened early, these children may be able to overcome some of their socio-behavioral handicaps. Hence, currently there is an endeavor to identify these children “at risk” early i.e., within first one to two years of life and introduce therapeutic interventions. This involves educating the parents about the red flags and also train the physicians to specifically ask and check for specific symptoms/ signs. This identification of autism spectrum disorders at an age of one to two years is referred to as “early identification” of autism\n",
    "Autism has an onset at an early age usually infancy, but it may go unnoticed because the signs are very subtle and non-specific. The important early cues that may aid diagnosis of autism include: Figure 1: Classification of ASD 41 Infancy (a) Infant not responding to cuddliness (b) Infant shying away from being picked up and held (c) Failure to make eye contact or look at people - especially at parents – when they are talking to them or interacting with them. (d) Failure to look at or acknowledge a toy or item when it is shown to them. Toddlers (e) Toddlers not engaging in imaginative play. Rather than pretending with their toys or other children, they may meticulously line up dolls, cars, and other items and become upset when someone disturbs the line. (f) Toddlers have difficulties sharing their toys/ items (g) Toddlers may not express any interest in a particular item even if that toy or doll is their favorite (h) As toddlers, thay may become fixated on specific objects or details of the objects, which may range from certain toys, certain items of clothing, or even the way sunlight hits the leaves outside the window. Spinning of wheels is a common obsession. But tactile obsessions may also be seen when the child may constantly rub soft or smooth items across their cheeks/ lips/ hand. (i) Toddler may be unresponsive. This means he/ she does not respond when his/ her name is called out. (j) Toddler may have repetitive action.They may watch the same movie over and over; rock back and forth continuously.\n",
    "Autism is quite different from most childhood ailments, for which, taking a blood sample or or doing an imaging test can determine the diagnosis. In autism, there is nothing organically wrong with the brain- it is not diseased or structurally different; rather autism is a malfunction in the way brain acts, so the only way to diagnose it is to observe a child’s behavior. Hence the diagnosis of autism is subjective, based on the answers a parent gives to a series of questions, and the observations of the evaluator. The one constant in a diagnosis is the criteria used: the Diagnostic and Statistical Manual of Mental Disorders, developed by the American Psychiatric Association and commonly referred to as DSM criteria. These DSM criteria have evolved over time, and currently DSM-5 criteria are used for diagnosing Autism Spectrum Disorders\n",
    "The fifth edition of Diagnostic and Statistical Manual of Mental Disorders (DSM-5) released in May 2013 does not sub-classify children according to Rett’s disorder, Childhood Disintegrative Disorder, Aspergers Disorder, or Pervasive Developmental Disorder (Not Otherwise Specified). It categorizes every child as ASD but with different levels of severity. These have been illustrated in table 1 and 2.\n",
    "The tests that are used for autism screening include : (a) CHecklist of Autism in Toddlers (CHAT) (b) Modified CHecklist of Autism in Toddlers (M-CHAT) (c) Screening Tool for Autism in Two-year olds (STAT) (d) Social Communication Questionnaire (SCQ)\n",
    "The tests that may be used in autism assessment include : 42 (a) Childhood Autism Rating Scale (CARS) (b) Autism Behavior Checklist (ABC) (c) Child Behavior Check List (CBCL) (d) Autism Treatment Evaluation Checklist (ATEC) (e) Developmental Profile 3\n",
    "Persistent deficits in social communication and social interaction across multiple contexts, as manifested by the following criteria, currently or by history : 1. Deficits in social-emotional reciprocity, ranging, for example, from abnormal social approach and failure of normal back-and-forth conversation; to reduced sharing of interests, emotions, or affect; to failure to initiate or respond to social interactions. 2. Deficits in non-verbal communicative behaviors used for social interaction, ranging, for example, from poorly integrated verbal and non-verbal communication; to abnormalities in eye contact and body language or deficits in understanding and use of gestures; to a total lack of facial expressions and non-verbal communication. 3. Deficits in developing, maintaining and understanding relationships, ranging, for example, from difficulties adjusting behavior to suit various social contexts; to difficulties in sharing imaginative play or in making friends; to absence of interest in peers. B. Restricted, repetitive patterns of behavior, interests or activities, as manifested by at least two of the following, currently or by history: 1. Stereotyped or repetitive motor movements, use of objects, or speech (example simple motor stereotypies, lining up toys or flipping objects, echolalia, idiosyncratic phrases). 2. Insistence on sameness, inflexible adherence to routines or ritualized patterns or verbal/ non-verbal behavior (e.g., extreme distress at small changes, difficulties with transitions, rigid thinking patterns, greeting rituals, need to take the same route or eat same food everyday) 3. Highly restricted, fixated interests that are abnormal in intensity or focus (e.g., strong attachment to or preoccupation with unusual objects, excessively circumscribed or preservative interest). 4. Hyper- or hypo- reactivity to sensory input or unusual interests in sensory aspects of the environment (e.g., apparent indifference to pain/ temperature, adverse response to specific sounds or textures, excessive smelling or touching of objects, visual fascination with lights or movements) C. Symptoms must be present in the early developmental period (but may not become fully manifested until social demands exceed limited capacities, or may be masked by learned strategies in later life). D. Symptoms cause clinically significant impairment in social, occupational or other important areas of current functioning. E. These disturbances are not better explained by intellectual disability (intellectual developmental disorder) or global developmental delay. Intellectual disability and autism spectrum disorder frequently co-occur; to make co-morbid diagnosis of autism spectrum disorder and intellectual disability, social communication should be below that expected for general developmental level\n",
    "\"\"\"\n",
    "\n",
    "print(wrapper.fill(context1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a53ce2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1733 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 1,733 tokens.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1733) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhat is Autism/ ASD?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext1\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36manswer_question\u001b[39m\u001b[34m(question, answer_text)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(segment_ids) == \u001b[38;5;28mlen\u001b[39m(input_ids)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# ======== Evaluate ========\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Run our example question through the model.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m start_scores, end_scores = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The tokens representing our input text.\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The segment IDs to differentiate question from answer_text\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Huggingface version update required the dictionary return to be False\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# ======== Reconstruct Answer ========\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Find the tokens with the highest `start` and `end` scores.\u001b[39;00m\n\u001b[32m     37\u001b[39m answer_start = torch.argmax(start_scores)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1955\u001b[39m, in \u001b[36mBertForQuestionAnswering.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1943\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1944\u001b[39m \u001b[33;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1945\u001b[39m \u001b[33;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1951\u001b[39m \u001b[33;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[32m   1952\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1953\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1956\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1961\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1962\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1963\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1965\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1967\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1969\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.qa_outputs(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1078\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1076\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1087\u001b[39m     attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:217\u001b[39m, in \u001b[36mBertEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.position_embedding_type == \u001b[33m\"\u001b[39m\u001b[33mabsolute\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    216\u001b[39m     position_embeddings = \u001b[38;5;28mself\u001b[39m.position_embeddings(position_ids)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\n\u001b[32m    218\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.LayerNorm(embeddings)\n\u001b[32m    219\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.dropout(embeddings)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1733) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "question = \"What is Autism/ ASD?\"\n",
    "\n",
    "answer_question(question, context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "834bf0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the features that may aid in early identification of autism?\n",
      "Answer: limited interests and/\n",
      "or repetitive behavior\n",
      "\n",
      "Input IDs: [101, 2054, 2024, 1996, 2838, 2008, 2089, 4681, 1999, 2220, 8720, 1997, 19465, 1029, 102, 2004, 2094, 4832, 2005, 19465, 8674, 8761, 1012, 1523, 19465, 1524, 1999, 1996, 18444, 5218, 2000, 1996, 18419, 1999, 4807, 1006, 4092, 1013, 22317, 1013, 5962, 1025, 2151, 5549, 1997, 4807, 2000, 2131, 1996, 4471, 2408, 2000, 2060, 2711, 1007, 1025, 2591, 3989, 1006, 2129, 1996, 3265, 16142, 2046, 1037, 2177, 2066, 1996, 2155, 1010, 2814, 2030, 2451, 1007, 1998, 3132, 5426, 1998, 1013, 2030, 23563, 5248, 1012, 2023, 2038, 2042, 9411, 1999, 6987, 1999, 4745, 20423, 2015, 1012, 1996, 1523, 8674, 1524, 2112, 1997, 18444, 14796, 2008, 2169, 1998, 2296, 2711, 2007, 2023, 4650, 2003, 2367, 1998, 4310, 1012, 2096, 2045, 2089, 2022, 2116, 5966, 2426, 2336, 1999, 2367, 3033, 1997, 8674, 1010, 1996, 3145, 4489, 2003, 2043, 2122, 2336, 2211, 2000, 2831, 1998, 4553, 1012, 2065, 2027, 2024, 3331, 2011, 1996, 2287, 1997, 1017, 2086, 1998, 2079, 2025, 2031, 7669, 1999, 4083, 1010, 1996, 2775, 2089, 2022, 18251, 2000, 2031, 2004, 4842, 4590, 1521, 1055, 1010, 2096, 2500, 2007, 8394, 2653, 19199, 2015, 1998, 18234, 4083, 2052, 2203, 2039, 2006, 1996, 5729, 2203, 1997, 8674, 1012, 1996, 3041, 16233, 2213, 4921, 9181, 4081, 1996, 2206, 4942, 1011, 5579, 1997, 19465, 1025, 2174, 1010, 1996, 3522, 16233, 2213, 1011, 1019, 2038, 5314, 2122, 1998, 4632, 27605, 13776, 2068, 2104, 1996, 12977, 2744, 1997, 2004, 2094, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Define the context and questions\n",
    "context = \"\"\"ASD stands for Autism Spectrum Disorder. “Autism” in the terminology refers to the defects in communication\n",
    "(speaking/ gesturing/ listening; any mode of communication to get the message across to other person);\n",
    "socialization (how the individual fits into a group like the family, friends or community) and limited interests and/\n",
    "or repetitive behavior. This has been dealt in detail in subsequent paragraphs.\n",
    "The “spectrum” part of terminology denotes that each and every person with this condition is different and\n",
    "unique. While there may be many differences among children in different parts of spectrum, the key difference\n",
    "is when these children began to talk and learn. If they are talking by the age of 3 years and do not have difficulty\n",
    "in learning, the child may be labelled to have Asperger’s, while others with delayed language milestones and\n",
    "impaired learning would end up on the severe end of spectrum. The earlier DSM IV criteria suggested the\n",
    "following sub-classification of autism; however, the recent DSM-5 has merged these and assimilated them\n",
    "under the umbrella term of ASD\"\"\"\n",
    "questions = [\n",
    "    \"What are the features that may aid in early identification of autism?\",\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "# Load the question-answering pipeline\n",
    "nlp = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad', tokenizer='bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Process each question and get answers\n",
    "for question in questions:\n",
    "    answer = nlp({\n",
    "        'question': question,\n",
    "        'context': context\n",
    "    })\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer['answer']}\\n\")\n",
    "\n",
    "# Ensure you have the correct tokenizer for your model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Check tokenization and encoding\n",
    "input_ids = tokenizer.encode(questions[0], context, truncation=True, padding='max_length', max_length=512)\n",
    "print(\"Input IDs:\", input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9e2ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f134296677384d808aa129ff4bf689b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\91809\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020ac63eb8c34d6c9b18fbd003fcd20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafabe4487d4451c933a10e9b88c24f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the usual age for the diagnosis for autism and what is early identification?\n",
      "Answer: 3 years\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"ASD stands for Autism Spectrum Disorder. “Autism” in the terminology refers to the defects in communication\n",
    "(speaking/ gesturing/ listening; any mode of communication to get the message across to other person);\n",
    "socialization (how the individual fits into a group like the family, friends or community) and limited interests and/\n",
    "or repetitive behavior. This has been dealt in detail in subsequent paragraphs.\n",
    "The “spectrum” part of terminology denotes that each and every person with this condition is different and\n",
    "unique. While there may be many differences among children in different parts of spectrum, the key difference\n",
    "is when these children began to talk and learn. If they are talking by the age of 3 years and do not have difficulty\n",
    "in learning, the child may be labelled to have Asperger’s, while others with delayed language milestones and\n",
    "impaired learning would end up on the severe end of spectrum. The earlier DSM IV criteria suggested the\n",
    "following sub-classification of autism; however, the recent DSM-5 has merged these and assimilated them\n",
    "under the umbrella term of ASD\"\"\"\n",
    "\n",
    "questions = [\"What is the usual age for the diagnosis for autism and what is early identification?\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9e3fcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kusha\\anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kusha\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the usual age for the diagnosis for autism and what is early identification?\n",
      "Answer: one to two years\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"Even though ASD can be diagnosed as early as age 2 years, most children are not diagnosed with ASD until\n",
    "after age 4 years. The median age of first diagnosis by subtype is as follows :\n",
    "• Autistic disorder : 3 years 10 months\n",
    "• Pervasive developmental disorder-not otherwise specified (PDD-NOS): 4 years 1 month\n",
    "• Asperger disorder : 6 years 2 months\n",
    "Studies have shown that parents of children with ASD notice a developmental problem before their child’s first\n",
    "birthday. Concerns about vision and hearing were more often reported in the first year, and differences in social,\n",
    "communication, and fine motor skills were evident from 6 months of age. And further research has shown that if\n",
    "intervened early, these children may be able to overcome some of their socio-behavioral handicaps. Hence,\n",
    "currently there is an endeavor to identify these children “at risk” early i.e., within first one to two years of life and\n",
    "introduce therapeutic interventions. This involves educating the parents about the red flags and also train the\n",
    "physicians to specifically ask and check for specific symptoms/ signs. This identification of autism spectrum\n",
    "disorders at an age of one to two years is referred to as “early identification” of autism\"\"\"\n",
    "\n",
    "questions = [\"What is the usual age for the diagnosis for autism and what is early identification?\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5490b31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the features that may aid in early identification of autism?\n",
      "Answer: infant not responding to cuddliness ( b ) infant shying away from being picked up and held\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"Autism has an onset at an early age usually infancy, but it may go unnoticed because the signs are very subtle\n",
    "and non-specific. The important early cues that may aid diagnosis of autism include:\n",
    "Figure 1: Classification of ASD\n",
    "Infancy\n",
    "(a) Infant not responding to cuddliness\n",
    "(b) Infant shying away from being picked up and held\n",
    "(c) Failure to make eye contact or look at people - especially at parents – when they are talking to them or\n",
    "interacting with them.\n",
    "(d) Failure to look at or acknowledge a toy or item when it is shown to them.\n",
    "Toddlers\n",
    "(e) Toddlers not engaging in imaginative play. Rather than pretending with their toys or other children, they\n",
    "may meticulously line up dolls, cars, and other items and become upset when someone disturbs the line.\n",
    "(f) Toddlers have difficulties sharing their toys/ items\n",
    "(g) Toddlers may not express any interest in a particular item even if that toy or doll is their favorite\n",
    "(h) As toddlers, thay may become fixated on specific objects or details of the objects, which may range from\n",
    "certain toys, certain items of clothing, or even the way sunlight hits the leaves outside the window. Spinning\n",
    "of wheels is a common obsession. But tactile obsessions may also be seen when the child may constantly\n",
    "rub soft or smooth items across their cheeks/ lips/ hand.\n",
    "(i) Toddler may be unresponsive. This means he/ she does not respond when his/ her name is called out.\n",
    "(j) Toddler may have repetitive action.They may watch the same movie over and over; rock back and forth\n",
    "continuously\"\"\"\n",
    "\n",
    "questions = [\"What are the features that may aid in early identification of autism?\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7563c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How is autism diagnosed?\n",
      "Answer: subjective\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"Autism is quite different from most childhood ailments, for which, taking a blood sample or or doing an imaging\n",
    "test can determine the diagnosis. In autism, there is nothing organically wrong with the brain- it is not diseased\n",
    "or structurally different; rather autism is a malfunction in the way brain acts, so the only way to diagnose it is to\n",
    "observe a child’s behavior. Hence the diagnosis of autism is subjective, based on the answers a parent gives to\n",
    "a series of questions, and the observations of the evaluator. The one constant in a diagnosis is the criteria used:\n",
    "the Diagnostic and Statistical Manual of Mental Disorders, developed by the American Psychiatric Association\n",
    "and commonly referred to as DSM criteria. These DSM criteria have evolved over time, and currently DSM-5\n",
    "criteria are used for diagnosing Autism Spectrum Disorders.\"\"\"\n",
    "\n",
    "questions = [\"How is autism diagnosed?\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce223fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the DSM-5 criteria for diagnosing ASD?\n",
      "Answer: different levels of severity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"The fifth edition of Diagnostic and Statistical Manual of Mental Disorders (DSM-5) released in May 2013 does\n",
    "not sub-classify children according to Rett’s disorder, Childhood Disintegrative Disorder, Aspergers Disorder, or\n",
    "Pervasive Developmental Disorder (Not Otherwise Specified). It categorizes every child as ASD but with different\n",
    "levels of severity. These have been illustrated in table 1 and 2\"\"\"\n",
    "\n",
    "questions = [\"What are the DSM-5 criteria for diagnosing ASD?\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5281d2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the other tests that are used for autism screening?\n",
      "Answer: social communication questionnaire ( scq )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"The tests that are used for autism screening include :\n",
    "CHecklist of Autism in Toddlers (CHAT),\n",
    "Modified CHecklist of Autism in Toddlers (M-CHAT),\n",
    "Screening Tool for Autism in Two-year olds (STAT),\n",
    "Social Communication Questionnaire (SCQ)\"\"\"\n",
    "\n",
    "questions = [\"What are the other tests that are used for autism screening?\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a261761a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Sleep problems in Autism Spectrum disorders?\n",
      "Answer: sleep problems can cause stress, anxiety and depression in parents and caregivers of children with autism spectrum disorders. in a study at aiims, it was found that sleep problems affect upto 78 % of children with autism\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"Though behavioral symptoms affect the children and their families, difficulties with sleep may present unique\n",
    "problems because it has been found that children who sleep poorly at night are more likely to exhibit daytime\n",
    "behavior problems. Daytime sleepiness can also interfere with the outcome of the child’s educational and\n",
    "behavioral programs. Sleep problems in the child often disrupt the entire family’s sleep, potentially leading to\n",
    "added daytime stress and irritability for all family members especially if there is a typically developing sibling\n",
    "with normal sleep pattern. Moreover, improved sleep in children with autism result in improvement in social\n",
    "interaction, reduced insistence on sameness, and adaptation to new environments. Estimating the magnitude of\n",
    "sleep problems and characterizing them are essential prerequisites before planning appropriate interventions for\n",
    "these children.\n",
    "Sleep is an active physiologic brain process occupying 1/3rd of human life, important for broad range of brain\n",
    "functions namely memory, concentration, learning, mood and behavior. Sleep deprived child becomes tired,\n",
    "sick, irritated and has difficulties in learning and coping. Sleep problems can cause stress, anxiety and depression\n",
    "in parents and caregivers of children with autism spectrum disorders. In a study at AIIMS, it was found that\n",
    "sleep problems affect upto 78% of children with autism.\"\"\"\n",
    "\n",
    "questions = [\"Sleep problems in Autism Spectrum disorders?\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a792ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the general teaching strategies for autism?\n",
      "Answer: individualized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"Although knowing the general characteristics of ASD is helpful, teaching strategies for students with ASD still\n",
    "need to be individualized, and it is important for teachers to realize their expectations of their students. Children\n",
    "with ASD often have visual-spatial strengths. Knowing this, teachers can modify their instructional strategies in a\n",
    "number of ways. First, teachers should demonstrate and model expected skills. For example, if the teacher\n",
    "wants a student with ASD to place his school bottle at a particular spot when he gets to class in the morning, the\n",
    "teacher should demonstrate exactly how to do this.\n",
    "Next, teachers should provide visual schedules of the day’s events in a location easily seen by the student. A\n",
    "visual schedule can be written out and paired with picture symbols to increase understanding.\n",
    "Additionally, teachers should work to make eye contact with the student and expect to acquire the student’s\n",
    "attention. This can be challenging because students with ASD may have difficulty maintaining eye contact due to\n",
    "difficulties with modulating visual input. Close proximity with a verbal reminder — a gentle, “look at me” — can\n",
    "work well. However, even if the student is not looking directly at the teacher, the teacher should know that he or\n",
    "she may still be listening. Checking frequently for understanding can give the teacher assurance that the student\n",
    "is, in fact, paying attention.\n",
    "Teachers can adopt other strategies to increase motivation in students with ASD, such as allowing short breaks\n",
    "between teaching sessions and providing time for the student to be alone if needed.\"\"\"\n",
    "\n",
    "questions = [\"What are the general teaching strategies for autism?\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a90fad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: early signs of ASD\n",
      "Answer: the child may display poor or fleeting eye contact\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"The child may display poor or fleeting eye contact. This is often one of the earliest signs\n",
    "of ASD, as is a lack of responding to one’s name.\n",
    "The child may exhibit difficulty initiating or maintaining a conversation. Or, conversations\n",
    "may focus on a preferred topic of interest. Individuals with ASD may have difficulty understanding when to start or stop a conversation.\n",
    "Language that seems “scripted” or echoed from television, a movie, or previous interaction. Many educators and caregivers report children with ASD engaging in “movie talk”\n",
    "or “TV talk” which refers to the child verbally repeating scenes from a television show or\n",
    "preferred movie.\n",
    "The child may lack empathy. For example, the child may be unable to recognize when\n",
    "another person is distressed because of a broken toy or dropped ice cream on the floor.\n",
    "The child may have difficulty understanding nonverbal communication (e.g., eye-rolling\n",
    "suggesting negative response or thumbs-up as a sign of agreement). The child may\n",
    "not understand cues of yawning or looking away as a lack of interest in the topic of\n",
    "conversation.\"\"\"\n",
    "\n",
    "questions = [\"early signs of ASD\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a102980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can play therapy help overcome autism?\n",
      "Answer: to build social interaction and communication skills\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"Play therapy is exactly what it sounds like: learning through the process of play. Autistic children are more likely to play alone and repeat actions over and over, rather than engage in pretend play. Therefore, the goal of play therapy is to build social interaction and communication skills and, in the long run, to enhance children's ability to engage in new activities and symbolic play.2\n",
    "\n",
    "Both parents and siblings can take part in play therapy. You can start by connecting with your child through simple chase-and-tickle games, bubble blowing, or sensory activities such as swinging, sliding, or wriggling through a tube. As your child's abilities grow, you may be able to build toward taking turns in back-and-forth games, collaborative games, or even make-believe.\"\"\"\n",
    "\n",
    "questions = [\"How can play therapy help overcome autism?\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9cceb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: is my child autistic\n",
      "Answer: the child may display poor or fleeting eye contact\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\"The child may display poor or fleeting eye contact. This is often one of the earliest signs\n",
    "of ASD, as is a lack of responding to one’s name.\n",
    "The child may exhibit difficulty initiating or maintaining a conversation. Or, conversations\n",
    "may focus on a preferred topic of interest. Individuals with ASD may have difficulty understanding when to start or stop a conversation.\n",
    "Language that seems “scripted” or echoed from television, a movie, or previous interaction. Many educators and caregivers report children with ASD engaging in “movie talk”\n",
    "or “TV talk” which refers to the child verbally repeating scenes from a television show or\n",
    "preferred movie.\n",
    "The child may lack empathy. For example, the child may be unable to recognize when\n",
    "another person is distressed because of a broken toy or dropped ice cream on the floor.\n",
    "The child may have difficulty understanding nonverbal communication (e.g., eye-rolling\n",
    "suggesting negative response or thumbs-up as a sign of agreement). The child may\n",
    "not understand cues of yawning or looking away as a lack of interest in the topic of\n",
    "conversation.\"\"\"\n",
    "\n",
    "questions = [\"is my child autistic\"]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"])\n",
    "\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715c319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
